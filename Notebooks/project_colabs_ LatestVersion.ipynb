{"cells":[{"cell_type":"markdown","id":"9e81664f","metadata":{"id":"9e81664f"},"source":["## 0 - Setup and data preparation\n","This cell loads libraries, the dataset, performs preprocessing, and basic feature engineering before the custom features."]},{"cell_type":"code","execution_count":null,"id":"eaa0015f","metadata":{"id":"eaa0015f"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Project.ipynb\n","\n","Automatically generated by Colab.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1nCZClEDak_KN_ZUgPhhRQ3Bp8T3v4ywz\n","\n","Task1-4\n","\"\"\"\n","\n","# !pip install datasets\n","# !pip install python-dotenv\n","\n","from dotenv import load_dotenv\n","import os\n","from huggingface_hub import login\n","\n","load_dotenv()\n","\n","hf_token = os.getenv(\"HF_TOKEN\")\n","login(token=hf_token)\n","\n","from datasets import load_dataset\n","\n","# Login using e.g. `huggingface-cli login` to access this dataset\n","dataset = load_dataset(\"KFUPM-JRCAI/arabic-generated-abstracts\")\n","print(dataset)\n","\n","# Inspect column names and data types for one split (e.g., 'by_polishing')\n","print(\"\\nFeatures in 'by_polishing':\")\n","print(dataset['by_polishing'].features)\n","\n","# Check dataset info (shape, structure, statistics)\n","print(\"\\nDataset info for 'by_polishing':\")\n","print(dataset['by_polishing'])\n","\n","# Choose one split (e.g., by_polishing)\n","split1 = dataset[\"by_polishing\"]\n","\n","# Count human-written abstracts\n","num_human = len(split1[\"original_abstract\"])\n","\n","# Count AI-generated abstracts (4 per row)\n","num_ai = len(split1[\"allam_generated_abstract\"]) \\\n","       + len(split1[\"jais_generated_abstract\"]) \\\n","       + len(split1[\"llama_generated_abstract\"]) \\\n","       + len(split1[\"openai_generated_abstract\"])\n","\n","print(\"Number of human abstracts:\", num_human)\n","print(\"Number of AI-generated abstracts:\", num_ai)\n","\n","# Distribution ratio\n","total = num_human + num_ai\n","print(\"Human %:\", round(num_human / total * 100, 2))\n","print(\"AI %:\", round(num_ai / total * 100, 2))\n","\n","import pandas as pd\n","# Convert to pandas for easier checks\n","df = pd.DataFrame(split1)\n","\n","# 1. Missing values\n","print(\"Missing values per column:\")\n","print(df.isnull().sum())\n","print(\"_________________________________________\")\n","\n","# 2. Duplicates\n","print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n","\n","# Also check duplicates in each column separately\n","for col in df.columns:\n","    print(f\"Duplicates in column {col}: {df[col].duplicated().sum()}\")\n","print(\"_________________________________________\")\n","\n","\n","# 3. Inconsistencies: empty strings or only spaces\n","for col in df.columns:\n","    empty_count = df[col].apply(lambda x: str(x).strip() == \"\").sum()\n","    print(f\"Empty/blank values in column {col}: {empty_count}\")\n","\n","split2 = dataset[\"from_title\"]\n","\n","# Count human-written abstracts\n","num_human = len(split2[\"original_abstract\"])\n","\n","# Count AI-generated abstracts (4 per row)\n","num_ai = len(split2[\"allam_generated_abstract\"]) \\\n","       + len(split2[\"jais_generated_abstract\"]) \\\n","       + len(split2[\"llama_generated_abstract\"]) \\\n","       + len(split2[\"openai_generated_abstract\"])\n","\n","print(\"Number of human abstracts:\", num_human)\n","print(\"Number of AI-generated abstracts:\", num_ai)\n","\n","# Distribution ratio\n","total = num_human + num_ai\n","print(\"Human %:\", round(num_human / total * 100, 2))\n","print(\"AI %:\", round(num_ai / total * 100, 2))\n","\n","import pandas as pd\n","# Convert to pandas for easier checks\n","df = pd.DataFrame(split2)\n","\n","# 1. Missing values\n","print(\"Missing values per column:\")\n","print(df.isnull().sum())\n","print(\"_________________________________________\")\n","\n","# 2. Duplicates\n","print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n","\n","# Also check duplicates in each column separately\n","for col in df.columns:\n","    print(f\"Duplicates in column {col}: {df[col].duplicated().sum()}\")\n","print(\"_________________________________________\")\n","\n","\n","# 3. Inconsistencies: empty strings or only spaces\n","for col in df.columns:\n","    empty_count = df[col].apply(lambda x: str(x).strip() == \"\").sum()\n","    print(f\"Empty/blank values in column {col}: {empty_count}\")\n","\n","split3 = dataset[\"from_title_and_content\"]\n","\n","# Count human-written abstracts\n","num_human = len(split3[\"original_abstract\"])\n","\n","# Count AI-generated abstracts (4 per row)\n","num_ai = len(split3[\"allam_generated_abstract\"]) \\\n","       + len(split3[\"jais_generated_abstract\"]) \\\n","       + len(split3[\"llama_generated_abstract\"]) \\\n","       + len(split3[\"openai_generated_abstract\"])\n","\n","print(\"Number of human abstracts:\", num_human)\n","print(\"Number of AI-generated abstracts:\", num_ai)\n","\n","# Distribution ratio\n","total = num_human + num_ai\n","print(\"Human %:\", round(num_human / total * 100, 2))\n","print(\"AI %:\", round(num_ai / total * 100, 2))\n","\n","import pandas as pd\n","# Convert to pandas for easier checks\n","df = pd.DataFrame(split3)\n","\n","# 1. Missing values\n","print(\"Missing values per column:\")\n","print(df.isnull().sum())\n","print(\"_________________________________________\")\n","\n","# 2. Duplicates\n","print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n","\n","# Also check duplicates in each column separately\n","for col in df.columns:\n","    print(f\"Duplicates in column {col}: {df[col].duplicated().sum()}\")\n","print(\"_________________________________________\")\n","\n","\n","# 3. Inconsistencies: empty strings or only spaces\n","for col in df.columns:\n","    empty_count = df[col].apply(lambda x: str(x).strip() == \"\").sum()\n","    print(f\"Empty/blank values in column {col}: {empty_count}\")\n","\n","import re\n","import nltk\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.stem.isri import ISRIStemmer\n","from datasets import load_dataset\n","\n","# Download required NLTK resources\n","nltk.download('stopwords')\n","\n","# Check columns\n","print(df.head())\n","\n","#Define Arabic text cleaning functions\n","# Remove tashkeel (diacritics)\n","def remove_diacritics(text):\n","    arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n","    return re.sub(arabic_diacritics, '', text)\n","\n","# Normalize Arabic text\n","def normalize_arabic(text):\n","    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n","    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n","    text = re.sub(\"Ø¤\", \"Ùˆ\", text)\n","    text = re.sub(\"Ø¦\", \"ÙŠ\", text)\n","    text = re.sub(\"Ø©\", \"Ù‡\", text)\n","    text = re.sub(\"[^Ø€-Û¿ ]+\", \" \", text)  # remove non-Arabic chars\n","    return text\n","\n","# Initialize stopwords and stemmer\n","arabic_stopwords = set(stopwords.words(\"arabic\"))\n","stemmer = ISRIStemmer()\n","\n","# Full preprocessing pipeline\n","def preprocess_text(text):\n","    text = str(text)\n","    text = remove_diacritics(text)\n","    text = normalize_arabic(text)\n","    tokens = text.split()\n","    tokens = [w for w in tokens if w not in arabic_stopwords]\n","    tokens = [stemmer.stem(w) for w in tokens]\n","    return \" \".join(tokens)\n","\n","# Apply preprocessing\n","text_columns = [\n","    'original_abstract',\n","    'allam_generated_abstract',\n","    'jais_generated_abstract',\n","    'llama_generated_abstract',\n","    'openai_generated_abstract'\n","]\n","for col in text_columns:\n","    clean_col = col + \"_clean\"\n","    df[clean_col] = df[col].apply(preprocess_text)\n","print(\" Preprocessing complete! Here are the new columns:\")\n","print(df.columns)\n","df.head(2)\n","\n","# Task 2.2: Exploratory Data Analysis (EDA)\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from collections import Counter\n","import seaborn as sns\n","import numpy as np\n","\n","# Combine AI abstracts into one column\n","ai_texts = pd.concat([\n","    df['allam_generated_abstract_clean'],\n","    df['jais_generated_abstract_clean'],\n","    df['llama_generated_abstract_clean'],\n","    df['openai_generated_abstract_clean']\n","], axis=0).dropna().tolist()\n","\n","human_texts = df['original_abstract_clean'].dropna().tolist()\n","\n","# --- Statistical Analysis ---\n","def text_stats(texts):\n","    words = [w for txt in texts for w in txt.split()]\n","    avg_word_len = np.mean([len(w) for w in words])\n","    avg_sent_len = np.mean([len(txt.split()) for txt in texts])\n","    vocab = set(words)\n","    ttr = len(vocab) / len(words)\n","    return avg_word_len, avg_sent_len, ttr\n","\n","stats_human = text_stats(human_texts)\n","stats_ai = text_stats(ai_texts)\n","\n","print(\"\\n Statistical Summary:\")\n","print(f\"Human-written: Avg word len={stats_human[0]:.2f}, Avg sent len={stats_human[1]:.2f}, TTR={stats_human[2]:.3f}\")\n","print(f\"AI-generated : Avg word len={stats_ai[0]:.2f}, Avg sent len={stats_ai[1]:.2f}, TTR={stats_ai[2]:.3f}\")\n","\n","# --- N-gram Frequency ---\n","def plot_top_ngrams(texts, n=2, top_k=15):\n","    from sklearn.feature_extraction.text import CountVectorizer\n","    vec = CountVectorizer(ngram_range=(n, n))\n","    bag = vec.fit_transform(texts)\n","    sum_words = bag.sum(axis=0)\n","    freqs = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n","    freqs = sorted(freqs, key=lambda x: x[1], reverse=True)[:top_k]\n","    words, counts = zip(*freqs)\n","    plt.figure(figsize=(10,4))\n","    sns.barplot(x=list(counts), y=list(words))\n","    plt.title(f\"Top {top_k} {n}-grams â€“ {n}-grams for {'Human' if texts==human_texts else 'AI'} abstracts\")\n","    plt.show()\n","\n","print(\"\\nðŸ”¤ Top Bigrams for Human-written abstracts:\")\n","plot_top_ngrams(human_texts, n=2)\n","\n","print(\"\\nðŸ”¤ Top Bigrams for AI-generated abstracts:\")\n","plot_top_ngrams(ai_texts, n=2)\n","\n","import matplotlib.pyplot as plt\n","\n","df[\"human_length\"] = df[\"original_abstract\"].apply(lambda x: len(x.split()))\n","df[\"ai_length\"] = df[\"openai_generated_abstract\"].apply(lambda x: len(x.split()))\n","\n","plt.figure(figsize=(8,5))\n","plt.hist(df[\"human_length\"], bins=30, alpha=0.6, label=\"Human-written\", color='blue')\n","plt.hist(df[\"ai_length\"], bins=30, alpha=0.6, label=\"AI-generated\", color='orange')\n","plt.xlabel(\"Sentence Length (words)\")\n","plt.ylabel(\"Frequency\")\n","plt.title(\"Sentence Length Distribution\")\n","plt.legend()\n","plt.show()\n","\n","def type_token_ratio(text):\n","    words = text.split()\n","    return len(set(words)) / len(words) if words else 0\n","\n","df[\"human_ttr\"] = df[\"original_abstract\"].apply(type_token_ratio)\n","df[\"ai_ttr\"] = df[\"openai_generated_abstract\"].apply(type_token_ratio)\n","\n","plt.figure(figsize=(6,5))\n","plt.boxplot([df[\"human_ttr\"], df[\"ai_ttr\"]], labels=[\"Human\", \"AI\"])\n","plt.title(\"Vocabulary Richness (Typeâ€“Token Ratio)\")\n","plt.ylabel(\"TTR Score\")\n","plt.show()\n","\n","from collections import Counter\n","import pandas as pd\n","\n","human_words = \" \".join(df[\"original_abstract\"]).split()\n","ai_words = \" \".join(df[\"openai_generated_abstract\"]).split()\n","\n","human_freq = Counter(human_words)\n","ai_freq = Counter(ai_words)\n","\n","common_words = set(list(human_freq.keys())[:100]) & set(list(ai_freq.keys())[:100])\n","\n","data = []\n","for w in common_words:\n","    data.append((w, human_freq[w], ai_freq[w]))\n","\n","freq_df = pd.DataFrame(data, columns=[\"word\", \"human\", \"ai\"]).sort_values(\"human\", ascending=False)[:15]\n","\n","freq_df.plot(x=\"word\", kind=\"bar\", figsize=(10,5), title=\"Top Words: Human vs AI\", rot=45)\n","plt.ylabel(\"Frequency\")\n","plt.show()\n","\n","# Combine all splits into one df_human\n","splits = [\"by_polishing\", \"from_title\", \"from_title_and_content\"]\n","\n","df_human = pd.concat([dataset[s].to_pandas() for s in splits], ignore_index=True)\n","\n","dfs = []\n","\n","for split_name in [\"by_polishing\", \"from_title\", \"from_title_and_content\"]:\n","    split_df = dataset[split_name].to_pandas().copy()\n","    split_df[\"source_split\"] = split_name   # <-- Create column manually\n","    dfs.append(split_df)\n","\n","df_human = pd.concat(dfs, ignore_index=True)\n","\n","ai_rows = []\n","\n","for _, row in df_human.iterrows():\n","    ai_models = [\n","        (\"allam\", row[\"allam_generated_abstract\"]),\n","        (\"jais\", row[\"jais_generated_abstract\"]),\n","        (\"llama\", row[\"llama_generated_abstract\"]),\n","        (\"openai\", row[\"openai_generated_abstract\"]),\n","    ]\n","\n","    for model_name, text in ai_models:\n","        ai_rows.append({\n","            \"abstract_text\": text,\n","            \"source_split\": row[\"source_split\"],     # now this exists\n","            \"generated_by\": model_name,\n","            \"label\": 0  # AI\n","        })\n","\n","# Convert to dataframe\n","df_ai = pd.DataFrame(ai_rows)\n","\n","# Create human dataframe\n","df_h = pd.DataFrame({\n","    \"abstract_text\": df_human[\"original_abstract\"],\n","    \"source_split\": df_human[\"source_split\"],\n","    \"generated_by\": \"human\",\n","    \"label\": 1\n","})\n","\n","# Final unified dataset\n","df = pd.concat([df_h, df_ai], ignore_index=True)\n","\n","print(\"Final unified dataset shape:\", df.shape)\n","df.head(10)\n","\n","print(df.columns)\n","\n","# Inspect column names and data types for one split (e.g., 'by_polishing')\n","print(\"\\nFeatures in 'by_polishing':\")\n","print(dataset['by_polishing'].features)\n","\n","# Check dataset info (shape, structure, statistics)\n","print(\"\\nDataset info for 'by_polishing':\")\n","print(dataset['by_polishing'])\n","\n","num_human = df[df[\"label\"] == 1].shape[0]\n","num_ai = df[df[\"label\"] == 0].shape[0]\n","\n","total = num_human + num_ai\n","\n","print(\"\\n===== Target Variable Distribution =====\")\n","print(\"Human-written abstracts:\", num_human)\n","print(\"AI-generated abstracts:\", num_ai)\n","print(\"Human %:\", round(num_human / total * 100, 2))\n","print(\"AI %:\", round(num_ai / total * 100, 2))\n","\n","print(\"\\n===== Missing Values =====\")\n","print(df.isnull().sum())\n","\n","print(\"\\n===== Duplicate Rows =====\")\n","print(\"Total duplicate rows:\", df.duplicated().sum())\n","\n","print(\"\\n===== Duplicate values per column =====\")\n","for col in df.columns:\n","    print(f\"{col}: {df[col].duplicated().sum()}\")\n","\n","print(\"\\n===== Empty / Blank Values =====\")\n","for col in df.columns:\n","    empty_count = df[col].apply(lambda x: str(x).strip() == \"\").sum()\n","    print(f\"{col}: {empty_count}\")\n","\n","# Check columns\n","print(df.head())\n","\n","#Define Arabic text cleaning functions\n","# Remove tashkeel (diacritics)\n","def remove_diacritics(text):\n","    arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n","    return re.sub(arabic_diacritics, '', text)\n","\n","# Normalize Arabic text\n","def normalize_arabic(text):\n","    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n","    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n","    text = re.sub(\"Ø¤\", \"Ùˆ\", text)\n","    text = re.sub(\"Ø¦\", \"ÙŠ\", text)\n","    text = re.sub(\"Ø©\", \"Ù‡\", text)\n","    text = re.sub(\"[^Ø€-Û¿ ]+\", \" \", text)  # remove non-Arabic chars\n","    return text\n","\n","# Initialize stopwords and stemmer\n","arabic_stopwords = set(stopwords.words(\"arabic\"))\n","stemmer = ISRIStemmer()\n","\n","# Full preprocessing pipeline\n","def preprocess_text(text):\n","    text = str(text)\n","    text = remove_diacritics(text)\n","    text = normalize_arabic(text)\n","    tokens = text.split()\n","    tokens = [w for w in tokens if w not in arabic_stopwords]\n","    tokens = [stemmer.stem(w) for w in tokens]\n","    return \" \".join(tokens)\n","\n","# Apply preprocessing\n","text_columns = [\n","    'abstract_text',\n","    'source_split',\n","    'generated_by',\n","    'label',\n","]\n","\n","# Apply preprocessing on the unified abstract text column\n","df[\"abstract_text_clean\"] = df[\"abstract_text\"].apply(preprocess_text)\n","\n","print(\"Preprocessing complete! Here are the new columns:\")\n","print(df.columns)\n","\n","df.head(2)\n","\n","\"\"\"**FEATURES:**\"\"\"\n","\n","#important library\n","import re\n","import math\n","import numpy as np\n","import pandas as pd\n","import unicodedata\n","from collections import Counter\n","from datasets import load_dataset\n","import regex as re2\n","\n","#Helper functions\n","\n","\n","def simple_word_tokenize(text):\n","    \"\"\"\n","    Tokenize text into words / symbols with Arabic support.\n","    \"\"\"\n","    return re2.findall(r\"\\p{Arabic}+|\\w+|[^\\s\\w]\", text, flags=re2.VERSION1)\n","\n","def sentence_tokenize(text):\n","    \"\"\"\n","    Split text into sentences using Arabic/English punctuation.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return []\n","    parts = re.split(r'(?<=[\\.\\?\\!\\u061F\\u061B])\\s+', text)\n","    return [p.strip() for p in parts if p.strip()]\n","\n","def paragraph_tokenize(text):\n","    \"\"\"\n","    Split text into paragraphs based on double newlines.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return []\n","    paragraphs = re.split(r'\\s*\\n\\s*\\n\\s*|\\s*\\r\\n\\s*\\r\\n\\s*', text.strip())\n","    return [p.strip() for p in paragraphs if p.strip()]\n","\n","# Column names to use\n","original_text_columns = \"abstract_text\"\n","clean_text_columns = \"abstract_text_clean\"\n","\n","# =============================\n","# 1. Tokens (use clean text)\n","# =============================\n","df[\"tokens\"] = df[clean_text_columns].apply(\n","    lambda t: [tok for tok in simple_word_tokenize(t) if tok.strip()] if isinstance(t, str) else []\n",")\n","\n","# =============================\n","# 2. Words (use clean tokens only)\n","# =============================\n","df[\"words\"] = df[\"tokens\"].apply(\n","    lambda toks: [tok for tok in toks if re.search(r'\\w', tok)]\n",")\n","\n","# =============================\n","# 3. Sentences (use original_text_columns for accurate sentence boundary detection)\n","# =============================\n","df[\"sentences\"] = df[original_text_columns].apply(\n","    lambda t: sentence_tokenize(t)\n",")\n","\n","# =============================\n","# 4. Paragraphs (use original_text_columns to preserve original structural breaks)\n","# =============================\n","df[\"paragraphs\"] = df[original_text_columns].apply(\n","    lambda t: paragraph_tokenize(t)\n",")\n","\n","print(\"Feature engineering completed! Columns now:\")\n","print(df.columns)\n","df.head(2)\n","\n","# Column names to use\n","original_text_columns = \"abstract_text\"\n","clean_text_columns = \"abstract_text_clean\"\n"]},{"cell_type":"markdown","id":"8a4933aa","metadata":{"id":"8a4933aa"},"source":["## 1 - Feature 12: Number of short words / N"]},{"cell_type":"code","execution_count":null,"id":"cad09c01","metadata":{"id":"cad09c01"},"outputs":[],"source":["# Feature 12: Number of short words / N\n","feature_name = f'{clean_text_columns}_f012_num_short_words_over_N'\n","\n","def _short_word_ratio(words, max_length=2):\n","    \"\"\"Calculates the ratio of words shorter than or equal to max_length to the total number of words.\"\"\"\n","    total_words = len(words)\n","    if total_words == 0:\n","        return 0.0\n","\n","    # Count words where length is less than or equal to the defined maximum\n","    short_words_count = sum(1 for w in words if len(w) <= max_length)\n","\n","    return float(short_words_count) / total_words\n","\n","df[feature_name] = df[\"words\"].apply(_short_word_ratio)\n","\n","df.head(5)\n"]},{"cell_type":"markdown","id":"8e8755a0","metadata":{"id":"8e8755a0"},"source":["## 2 - Feature 35: Total number of paragraphs (P)"]},{"cell_type":"code","execution_count":null,"id":"f4c9f273","metadata":{"id":"f4c9f273"},"outputs":[],"source":["# 35. Total number of paragraphs (P)\n","df['f035_Total_number_of_paragraphs_(P)'] = df[\"paragraphs\"].apply(len)\n","\n","df.head(5)\n"]},{"cell_type":"markdown","id":"cd0cd191","metadata":{"id":"cd0cd191"},"source":["## 3 - Feature 58: Number of words among the top-100 most frequent words"]},{"cell_type":"code","execution_count":null,"id":"12b45409","metadata":{"id":"12b45409"},"outputs":[],"source":["# 58. Number of words found in the 100 most frequent positions\n","# (approximation using global frequency over the clean words column)\n","\n","from collections import Counter\n","\n","# Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ±Ø¯Ø¯Ø§Øª Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯ \"words\"\n","all_words = []\n","for words_list in df[\"words\"]:\n","    if isinstance(words_list, list):\n","        all_words.extend(words_list)\n","\n","counter = Counter(all_words)\n","top100_words = set([w for w, _ in counter.most_common(100)])  # Top 100 ÙƒÙ„Ù…Ø§Øª\n","\n","# Ù„Ø­Ø³Ø§Ø¨ Feature 58\n","# ------------------------------\n","def count_top100_embedding_words(tokens, top_words_set=top100_words):\n","    if not isinstance(tokens, list):\n","        return 0\n","    return sum(1 for tok in tokens if tok in top_words_set)\n","\n","# ØªØ·Ø¨ÙŠÙ‚ Feature 58 Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯\n","feature_58_name = f'{clean_text_columns}_f058_num_top100_embedding_words'\n","df[feature_58_name] = df[\"words\"].apply(count_top100_embedding_words)\n","\n","# Ù„ÙØ­Øµ Ù†ØªØ§Ø¦Ø¬ Ù…ÙŠØ²Ø© 58\n","df[[\"words\", feature_58_name]].head()\n","\n"]},{"cell_type":"code","source":["!pip install transformers -q\n"],"metadata":{"id":"L8sg39hcjSnX"},"id":"L8sg39hcjSnX","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"afd3a098","metadata":{"id":"afd3a098"},"source":["## 4 - Feature 81: Language-model-based LogRank score"]},{"cell_type":"code","execution_count":null,"id":"PAnaPwVlaWEa","metadata":{"id":"PAnaPwVlaWEa"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import pandas as pd\n","\n","# ØªØ£ÙƒØ¯ Ø¥Ù†Ùƒ Ø«Ø¨Ù‘ØªØª transformers ÙÙŠ Ø®Ù„ÙŠØ© ÙÙˆÙ‚:\n","# !pip install transformers -q\n","\n","model_name = \"distilgpt2\"  # Ù†Ø³Ø®Ø© Ø£Ø®Ù Ù…Ù† gpt2\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","model.eval()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def compute_logrank(text):\n","    if not isinstance(text, str) or not text.strip():\n","        return 0.0\n","\n","    inputs = tokenizer(\n","        text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=128  # Ù‚Ù„Ù‘Ù„ Ø§Ù„Ø·ÙˆÙ„ Ø¹Ø´Ø§Ù† Ø§Ù„Ø³Ø±Ø¹Ø©\n","    )\n","    input_ids = inputs[\"input_ids\"].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, labels=input_ids)\n","        log_rank_score = outputs.loss.item()\n","\n","    return log_rank_score\n","\n","# Ù†Ø¬Ù‡Ø² Ø§Ù„Ø¹Ù…ÙˆØ¯\n","feature_81_name = f\"{clean_text_columns}_f081_logrank\"\n","\n","values = []\n","max_rows = 1000   # Ù†Ø­Ø³Ø¨ Ù„Ø£ÙˆÙ„ 1000 Ø¹ÙŠÙ†Ø© ÙÙ‚Ø·ØŒ ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ù†Ø®Ù„ÙŠÙ‡ 0\n","for i, t in enumerate(df[clean_text_columns]):\n","    if i < max_rows and pd.notna(t):\n","        values.append(compute_logrank(t))\n","    else:\n","        values.append(0.0)\n","\n","df[feature_81_name] = values\n","\n","df[[feature_81_name]].head()"]},{"cell_type":"markdown","id":"5818dd2f","metadata":{"id":"5818dd2f"},"source":["## 5 - Feature 104: Root Pattern Frequency"]},{"cell_type":"code","execution_count":null,"id":"247c6ce3","metadata":{"id":"247c6ce3"},"outputs":[],"source":["# 104. Root Pattern Frequency\n","# ÙŠÙ‚ÙŠØ³ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù…Ø«Ù„ \"ÙØ¹Ù„\"ØŒ \"Ù…ÙØ¹ÙˆÙ„\") Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©.\n","\n","root_patterns = [\"ÙØ¹Ù„\", \"Ø§ÙØ¹Ø§Ù„\", \"Ù…ÙØ¹ÙˆÙ„\", \"ÙØ§Ø¹Ù„Ø©\"]  # Ù…Ø«Ø§Ù„ Ù„Ø£Ù†Ù…Ø§Ø· Ø¬Ø°ÙˆØ±\n","\n","def root_pattern_frequency(text):\n","    if not isinstance(text, str) or len(text.strip()) == 0:\n","        return 0.0\n","    count = sum(text.count(pattern) for pattern in root_patterns)\n","    total_words = len(simple_word_tokenize(text))\n","    return count / total_words if total_words > 0 else 0.0\n","\n","feature_104_name = f\"{original_text_columns}_f104_root_pattern_freq\"\n","df[feature_104_name] = df[original_text_columns].apply(root_pattern_frequency)\n","df[[original_text_columns, feature_104_name]].head()\n"]},{"cell_type":"markdown","id":"7a013681","metadata":{"id":"7a013681"},"source":["## 6 - Train/Validation/Test split and model training"]},{"cell_type":"code","source":["!pip -q install sentence-transformers xgboost"],"metadata":{"id":"nXUTt5wVq5l3"},"id":"nXUTt5wVq5l3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# First split: Train 70%, Temp 30%  (Ù…Ø¹ stratify Ø¹Ø´Ø§Ù† ØªÙˆØ²ÙŠØ¹ labels ÙŠØ¨Ù‚Ù‰ Ù…ØªÙˆØ§Ø²Ù†)\n","train_df, temp_df = train_test_split(\n","    df, test_size=0.30, random_state=42, shuffle=True, stratify=df[\"label\"]\n",")\n","\n","# Second split: Temp 30% -> 15% Val, 15% Test\n","val_df, test_df = train_test_split(\n","    temp_df, test_size=0.50, random_state=42, shuffle=True, stratify=temp_df[\"label\"]\n",")\n","\n","print(\"TOTAL:\", len(df))\n","print(\"TRAIN:\", len(train_df))\n","print(\"VAL:\", len(val_df))\n","print(\"TEST:\", len(test_df))\n","\n","print(\"\\nLabel distribution:\")\n","print(\"TRAIN:\\n\", train_df[\"label\"].value_counts(normalize=True))\n","print(\"VAL:\\n\", val_df[\"label\"].value_counts(normalize=True))\n","print(\"TEST:\\n\", test_df[\"label\"].value_counts(normalize=True))"],"metadata":{"id":"3jz7HnHZq-A5"},"id":"3jz7HnHZq-A5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorer = TfidfVectorizer(\n","    max_features=5000,\n","    ngram_range=(1, 2),\n","    analyzer=\"word\"\n",")\n","\n","# Fit ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n","tfidf_vectorer.fit(train_df[\"abstract_text_clean\"])\n","\n","# Transform\n","X_train_tfidf = tfidf_vectorer.transform(train_df[\"abstract_text_clean\"])\n","X_val_tfidf   = tfidf_vectorer.transform(val_df[\"abstract_text_clean\"])\n","X_test_tfidf  = tfidf_vectorer.transform(test_df[\"abstract_text_clean\"])\n","\n","print(\"TF-IDF shapes:\")\n","print(\"Train:\", X_train_tfidf.shape)\n","print(\"Validation:\", X_val_tfidf.shape)\n","print(\"Test:\", X_test_tfidf.shape)"],"metadata":{"id":"Ds6_mChvrANZ"},"id":"Ds6_mChvrANZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.sparse import hstack, csr_matrix\n","\n","EXCLUDED_COLS = [\n","    \"label\", \"abstract_text\", \"abstract_text_clean\",\n","    \"tokens\", \"words\", \"sentences\", \"paragraphs\", \"abstract_text_pos_tags\"\n","]\n","\n","# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙÙ‚Ø· (Ø§Ù„ÙÙŠØªØ´Ø±Ø² Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¹ØªÙ‡Ø§)\n","numeric_cols = [\n","    col for col in train_df.select_dtypes(include=np.number).columns\n","    if col not in EXCLUDED_COLS\n","]\n","\n","X_train_num = csr_matrix(train_df[numeric_cols].to_numpy())\n","X_val_num   = csr_matrix(val_df[numeric_cols].to_numpy())\n","X_test_num  = csr_matrix(test_df[numeric_cols].to_numpy())\n","\n","y_train = train_df[\"label\"].to_numpy()\n","y_val   = val_df[\"label\"].to_numpy()\n","y_test  = test_df[\"label\"].to_numpy()\n","\n","# Ø¯Ù…Ø¬ Sparse + Sparse (Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„)\n","X_train = hstack([X_train_tfidf, X_train_num]).tocsr()\n","X_val   = hstack([X_val_tfidf,   X_val_num]).tocsr()\n","X_test  = hstack([X_test_tfidf,  X_test_num]).tocsr()\n","\n","print(\"X and y are ready for ML models.\")\n","print(\"Train:\", X_train.shape, y_train.shape)\n","print(\"Validation:\", X_val.shape, y_val.shape)\n","print(\"Test:\", X_test.shape, y_test.shape)\n","\n","print(\"\\nNumeric feature count:\", len(numeric_cols))"],"metadata":{"id":"8FdJYQGLrDvI"},"id":"8FdJYQGLrDvI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","lr_model = LogisticRegression(max_iter=2000, random_state=42, solver=\"liblinear\")\n","lr_model.fit(X_train, y_train)\n","\n","y_val_pred = lr_model.predict(X_val)\n","print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n","print(classification_report(y_val, y_val_pred))\n","\n","y_test_pred = lr_model.predict(X_test)\n","print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n","print(classification_report(y_test, y_test_pred))\n","\n","cm = confusion_matrix(y_test, y_test_pred)\n","plt.figure(figsize=(4,4))\n","plt.imshow(cm)\n","plt.title(\"Confusion Matrix - Logistic Regression\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","for (i, j), v in np.ndenumerate(cm):\n","    plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n","plt.show()"],"metadata":{"id":"HbKl0rzLrK0K"},"id":"HbKl0rzLrK0K","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","import xgboost as xgb\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","models = {}\n","\n","# SVM (LinearSVC Ø£Ø³Ø±Ø¹ Ù…Ø¹ sparse)\n","svm_model = LinearSVC(C=1.0, random_state=42)\n","svm_model.fit(X_train, y_train)\n","models[\"SVM\"] = svm_model\n","\n","# Random Forest (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø·ÙŠØ¡ Ù…Ø¹ features ÙƒØ«ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§)\n","rf_model = RandomForestClassifier(\n","    n_estimators=200,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_model.fit(X_train, y_train)\n","models[\"RandomForest\"] = rf_model\n","\n","# XGBoost\n","xgb_model = xgb.XGBClassifier(\n","    n_estimators=200,\n","    max_depth=6,\n","    learning_rate=0.1,\n","    eval_metric=\"logloss\",\n","    random_state=42,\n","    n_jobs=-1\n",")\n","xgb_model.fit(X_train, y_train)\n","models[\"XGBoost\"] = xgb_model\n","\n","# Validation quick check\n","for name, model in models.items():\n","    yv = model.predict(X_val)\n","    print(f\"\\n{name} Validation Accuracy:\", accuracy_score(y_val, yv))\n","    print(classification_report(y_val, yv))\n","\n","# Test evaluation + confusion matrix\n","for name, model in models.items():\n","    yt = model.predict(X_test)\n","    print(f\"\\n===== {name} Test Evaluation =====\")\n","    print(\"Accuracy:\", accuracy_score(y_test, yt))\n","    print(classification_report(y_test, yt))\n","\n","    cm = confusion_matrix(y_test, yt)\n","    plt.figure(figsize=(4,4))\n","    plt.imshow(cm)\n","    plt.title(f\"Confusion Matrix - {name}\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"Actual\")\n","    for (i, j), v in np.ndenumerate(cm):\n","        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n","    plt.show()"],"metadata":{"id":"Qssom31GrNpo"},"id":"Qssom31GrNpo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from tensorflow.keras import layers, models\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","bert_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n","\n","X_train_emb = bert_model.encode(train_df[\"abstract_text_clean\"].tolist(), convert_to_numpy=True)\n","X_val_emb   = bert_model.encode(val_df[\"abstract_text_clean\"].tolist(), convert_to_numpy=True)\n","X_test_emb  = bert_model.encode(test_df[\"abstract_text_clean\"].tolist(), convert_to_numpy=True)\n","\n","y_train_nn = train_df[\"label\"].to_numpy()\n","y_val_nn   = val_df[\"label\"].to_numpy()\n","y_test_nn  = test_df[\"label\"].to_numpy()\n","\n","ffnn_model = models.Sequential([\n","    layers.Input(shape=(X_train_emb.shape[1],)),\n","    layers.Dense(256, activation=\"relu\"),\n","    layers.Dropout(0.3),\n","    layers.Dense(128, activation=\"relu\"),\n","    layers.Dropout(0.3),\n","    layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","ffnn_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","ffnn_model.summary()\n","\n","history = ffnn_model.fit(\n","    X_train_emb, y_train_nn,\n","    validation_data=(X_val_emb, y_val_nn),\n","    epochs=10,\n","    batch_size=32,\n","    verbose=1\n",")\n","\n","# Predict (ØªØµØ­ÙŠØ­: ravel)\n","y_test_pred_nn = (ffnn_model.predict(X_test_emb).ravel() > 0.5).astype(int)\n","\n","print(\"FFNN Test Accuracy:\", accuracy_score(y_test_nn, y_test_pred_nn))\n","print(\"Classification Report (FFNN Test):\")\n","print(classification_report(y_test_nn, y_test_pred_nn))"],"metadata":{"id":"q8q3LMxDrQ9Z"},"id":"q8q3LMxDrQ9Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, joblib\n","from tensorflow.keras.models import Model as KerasModel\n","\n","def save_all_models(models_dict, save_dir=\"models\"):\n","    \"\"\"\n","    Saves all ML/DL models to disk based on their type.\n","\n","    Parameters:\n","    -----------\n","    models_dict : dict\n","        Example:\n","            {\n","                \"logistic_regression\": log_reg_model,\n","                \"svm\": svm_model,\n","                \"random_forest\": rf_model,\n","                \"xgboost\": xgb_model,\n","                \"ffnn\": ffnn_model\n","            }\n","\n","    save_dir : str\n","        Directory where models will be saved.\n","    \"\"\"\n","\n","    # Create save folder\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    for model_name, model_obj in models_dict.items():\n","\n","        # Case 1 â€” Keras deep learning model\n","        if isinstance(model_obj, KerasModel):\n","            file_path = os.path.join(save_dir, f\"{model_name}.h5\")\n","            model_obj.save(file_path)\n","            print(f\"[Saved] Keras model â†’ {file_path}\")\n","\n","        # Case 2 â€” All pickle-compatible models (Sklearn, XGBoost)\n","        else:\n","            file_path = os.path.join(save_dir, f\"{model_name}.pkl\")\n","            joblib.dump(model_obj, file_path)\n","            print(f\"[Saved] Pickle model â†’ {file_path}\")\n","\n","    print(\"\\nAll models saved successfully!\")"],"metadata":{"id":"YmGGpPAlrUA4"},"id":"YmGGpPAlrUA4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cc5b7e21"},"source":["import os\n","import joblib\n","from tensorflow.keras.models import load_model as keras_load_model\n","\n","def load_all_models(save_dir=\"models\"):\n","    \"\"\"\n","    Loads all ML/DL models from disk.\n","\n","    Parameters:\n","    -----------\n","    save_dir : str\n","        Directory where models are saved.\n","\n","    Returns:\n","    --------\n","    dict\n","        A dictionary containing the loaded models, with model names as keys.\n","    \"\"\"\n","    loaded_models = {}\n","    if not os.path.exists(save_dir):\n","        print(f\"Error: Directory '{save_dir}' not found.\")\n","        return loaded_models\n","\n","    for filename in os.listdir(save_dir):\n","        model_name, ext = os.path.splitext(filename)\n","        file_path = os.path.join(save_dir, filename)\n","\n","        if ext == '.h5':  # Keras model\n","            try:\n","                loaded_models[model_name] = keras_load_model(file_path)\n","                print(f\"[Loaded] Keras model â†’ {file_path}\")\n","            except Exception as e:\n","                print(f\"Error loading Keras model {filename}: {e}\")\n","        elif ext == '.pkl':  # Pickle-compatible model (Sklearn, XGBoost)\n","            try:\n","                loaded_models[model_name] = joblib.load(file_path)\n","                print(f\"[Loaded] Pickle model â†’ {file_path}\")\n","            except Exception as e:\n","                print(f\"Error loading pickle model {filename}: {e}\")\n","        else:\n","            print(f\"Skipping unknown file type: {filename}\")\n","\n","    print(\"\\nAll models loaded successfully!\")\n","    return loaded_models\n","\n","# Load all models\n","loaded_models = load_all_models()\n","\n","\n","# lr_model_loaded = loaded_models['lr_model']\n","# svm_model_loaded = loaded_models['svm']\n","# rf_model_loaded = loaded_models['random_forest']\n","# xgb_model_loaded = loaded_models['xgboost']\n","# ffnn_model_loaded = loaded_models['ffnn']\n"],"id":"cc5b7e21","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9670dafb"},"source":["### Verification: List Loaded Models\n","\n"],"id":"9670dafb"},{"cell_type":"code","metadata":{"id":"bfba63ed"},"source":["print(\"Models successfully loaded:\")\n","for model_name in loaded_models.keys():\n","    print(f\"- {model_name}\")\n"],"id":"bfba63ed","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1BFFebDhamj1xQ8YfoW7HxgEwKmIbto6X","timestamp":1765901887104},{"file_id":"1wK1vIjZYZGEDj5RnkRrh5dYUPGB5NY90","timestamp":1765876221439}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}